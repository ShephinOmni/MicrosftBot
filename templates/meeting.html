<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" rel="stylesheet">
    <title>Video Call Interface</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
        }

        .video-call-container {
            margin-bottom: 20px;
            text-align: center;
            padding: 10px;
            height: 550px; /* Adjusted height */
        }

        .video-box {
            margin-bottom: 20px;
            text-align: center;
            padding: 10px;
            height: 550px; /* Adjusted height */
        }
        video, img {
            max-width: 100%;
            height: 100%; /* Set height to fill the container */
            object-fit: cover; /* This will ensure the video covers the full area, might crop a bit */
        }

        .video-window {
            width: 300px;
            height: 200px;
            margin: 10px;
            border: 1px solid #ddd;
            border-radius: 8px;
            overflow: hidden;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .controls {
            display: flex;
            justify-content: space-between;
            width: 100%;
        }

        .control-button {
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 4px;
            padding: 10px 20px;
            margin: 10px;
            cursor: pointer;
            font-size: 16px;
        }

        .chat-container {
            display: flex;
            width: 100%;
        }

        .chat-box {
            flex: 1;
            margin: 10px;
            padding: 10px;
            background-color: #f5f5f5;
            border: 1px solid #ddd;
            border-radius: 4px;
            min-height: 100px;
            font-size: 18px;
        }

        .subtitles {
            position: absolute; /* Position it over the video */
            bottom: 20px; /* Position it at the bottom of the video */
            left: 0;
            width: 100%;
            text-align: center;
            padding: 10px;
            background-color: rgba(0, 0, 0, 0.5); /* Semi-transparent background */
            color: white;
            font-size: 20px;
            box-sizing: border-box; /* To include padding and border in the element's total width and height */
            overflow: hidden; /* To handle overflow text */
            white-space: nowrap; /* To prevent text wrapping */
            text-overflow: ellipsis; /* To indicate text overflow with an ellipsis */
        }

        .video-box {
            position: relative; /* Required for absolute positioning of children */
            /* Rest of your styles for video-box */
        }

        /* Additional styles for the ChatGPT response container */
        #chatgptResponse {
            bottom: 20px; /* Position it at the top of the video box */
            bottom: auto; /* Override the bottom property from .subtitles */
            background-color: rgba(50, 50, 50, 0.7); /* Different background color for distinction */
        }
    </style>
</head>
<body>
    <div class="container mt-4">
        <div class="row">
            <div class="col-md-6">
                <div class="video-box">
                    <video id="userCamera" autoplay playsinline muted style="width: 100%; position: relative;"></video>
                    <div id="subtitles" class="subtitles"></div> <!-- Subtitles div -->
                </div>
            </div>
            <div class="col-md-6">
                <div class="video-box">
                    <img src="https://i.gifer.com/QjoV.gif" alt="AI Avatar" style="width: 100%;">
                    <div id="chatgptResponse" class="subtitles"></div> <!-- ChatGPT Response Subtitles div -->
                </div>
            </div>
        </div>

        <div class="controls">
            <div class="icons-bar">
                <button id="camera-toggle" class="btn icon" style="background-color: red;">
                    <i class="fas fa-video"></i>
                </button>
                <button id="microphone-toggle" class="btn icon" style="background-color: red;">
                    <i class="fas fa-microphone-slash"></i>
                </button>
                <button id="exit-button" class="btn icon" style="background-color: red;">
                    <i class="fas fa-sign-out-alt"></i>
                </button>
            </div>
        </div>
    </div>
<script>
document.addEventListener('DOMContentLoaded', function() {
    var videoElement = document.getElementById('userCamera');
    var cameraButton = document.getElementById('camera-toggle');
    var microphoneButton = document.getElementById('microphone-toggle');
    var isMicActive = false;
    var isCameraActive = true; // Camera is on by default
    var recognition;
    var lastTranscript = '';
    var speechTimeout;
    
    // Access camera
    function accessCamera() {
        navigator.mediaDevices.getUserMedia({ video: true, audio: false })
            .then(newStream => {
                stream = newStream;
                videoElement.srcObject = stream;
                videoElement.onloadedmetadata = () => videoElement.play();
                updateCameraIcon(true);
            }).catch(error => {
                console.error("Error accessing the camera: ", error);
                alert("Error accessing the camera.");
                updateCameraIcon(false);
            });
    }

    // Toggle camera
    function toggleCamera() {
        if (stream && isCameraActive) {
            stream.getTracks().forEach(track => track.stop());
            stream = null;
            updateCameraIcon(false);
            isCameraActive = false;
        } else {
            accessCamera();
            isCameraActive = true;
        }
    }

    // Update camera icon
    function updateCameraIcon(isCameraOn) {
        const cameraIcon = cameraButton.querySelector('i');
        if (isCameraOn) {
            cameraIcon.classList.replace('fa-video-slash', 'fa-video');
        } else {
            cameraIcon.classList.replace('fa-video', 'fa-video-slash');
        }
    }

    // Initialize camera on page load
    accessCamera();


    // Function to update the microphone icon
    function updateMicrophoneIcon(isActive) {
            const micIcon = microphoneButton.querySelector('i');
            micIcon.classList.toggle('fa-microphone-slash', !isActive);
            micIcon.classList.toggle('fa-microphone', isActive);
        }

    // Function to initialize speech recognition
    function initializeSpeechRecognition() {
        recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        recognition.interimResults = true;
        recognition.continuous = true;

        recognition.onresult = event => {
            // Reset the timer whenever new speech is recognized
            clearTimeout(speechTimeout);

            // Update the subtitle text
            lastTranscript = Array.from(event.results)
                .map(result => result[0])
                .map(result => result.transcript)
                .join('');
            document.getElementById('subtitles').innerText = lastTranscript;

            // Set a new timer to clear the subtitle text after 2 seconds of inactivity
            speechTimeout = setTimeout(() => {
                document.getElementById('subtitles').innerText = ''; // Clear the subtitles
            }, 2000); // 2-second pause
        };
    }

    // Initialize speech recognition
    initializeSpeechRecognition();



    function updateSubtitles(newText) {
        const maxWords = 10;  // Set the maximum number of words to display
        const subtitleDiv = document.getElementById('subtitles');
        let words = subtitleDiv.innerText.split(' ');

        // Add new words to the array
        words = words.concat(newText.split(' '));

        // Keep only the last 'maxWords' words
        words = words.slice(Math.max(words.length - maxWords, 0));

        // Update the subtitle text
        subtitleDiv.innerText = words.join(' ');
    }

    function playAndTranscribeAudioResponse(audioUrl) {
        let audio = new Audio(audioUrl);
        let chatGptRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        chatGptRecognition.lang = 'en-US';
        chatGptRecognition.interimResults = true;
        chatGptRecognition.continuous = true;

        chatGptRecognition.onresult = function(event) {
            let transcript = Array.from(event.results)
                                  .map(result => result[0])
                                  .map(result => result.transcript)
                                  .join(' ');
            updateChatGptResponse(transcript);
        };

        chatGptRecognition.onerror = function(event) {
            console.error('Speech recognition error:', event.error);
        };

        audio.play();
        chatGptRecognition.start();

        audio.onended = function() {
            chatGptRecognition.stop(); // Stop recognition when audio ends
            URL.revokeObjectURL(audioUrl); // Clean up
        };
    }


    // Function to toggle microphone
    function toggleMicrophone() {
            if (!isMicActive) {
                initializeSpeechRecognition();
                recognition.start();
                lastTranscript = '';
            } else {
                recognition.stop();
                if (lastTranscript.trim().length > 0) {
                    sendTextToServer(lastTranscript);
                }
            }
            isMicActive = !isMicActive;
            updateMicrophoneIcon(isMicActive);
        }
        

    // Function to handle the end of speech input
    function handleSpeechEnd() {
        clearTimeout(speechPauseTimer);
        speechPauseTimer = setTimeout(() => {
            sendTextToServer(lastTranscript); // Send the last transcript to the server
            lastTranscript = ''; // Reset the transcript
            document.getElementById('subtitles').innerText = ''; // Clear the subtitles
        }, 2000); // 2-second pause
    }

    // Modified recognition.onresult function
    recognition.onresult = event => {
        clearTimeout(speechPauseTimer); // Clear existing timer
        const transcript = Array.from(event.results)
            .map(result => result[0])
            .map(result => result.transcript)
            .join('');
            lastTranscript = Array.from(event.results)
            .map(result => result[0])
            .map(result => result.transcript)
            .join('');
        document.getElementById('subtitles').innerText = transcript;
        handleSpeechEnd(); // Start/reset the pause timer
    };

    function updateChatGptResponse(newText) {
        const chatGptResponseDiv = document.getElementById('chatgptResponse');
        chatGptResponseDiv.innerText = newText; // Update with real-time transcript
    }


    


    // function playAudioResponse(audioUrl) {
    //     let audio = new Audio(audioUrl);
    //     audio.play();
    //     transcribeAudioResponse(audio);
    //     audio.onended = function() {
    //         URL.revokeObjectURL(audioUrl); // Clean up after playback
    //     };
    //     // Start transcribing ChatGPT's audio response
    //     transcribeChatGptResponse();
    // }

    function updateSubtitles(newText, maxWords = 10) {
        const divToUpdate = document.getElementById('subtitles');
        let words = newText.split(' ');
        if (divToUpdate.innerText) {
            words = divToUpdate.innerText.split(' ').concat(words);
        }
        divToUpdate.innerText = words.slice(-maxWords).join(' ');
    }

    // Transcribe the audio response using the Web Speech API
    function transcribeAudioResponse(audioElement) {
        let recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        recognition.lang = 'en-US'; // Set language
        recognition.interimResults = false; // Only final results
        recognition.maxAlternatives = 1; // Only one alternative

        recognition.onresult = function(event) {
            let transcript = event.results[0][0].transcript;
            updateChatGptResponse(transcript);
        };

        recognition.onerror = function(event) {
            console.error('Speech recognition error', event.error);
        };

        // Start recognition when the audio plays
        audioElement.onplay = function() {
            recognition.start();
        };

        // Stop recognition when the audio ends
        audioElement.onended = function() {
            recognition.stop();
        };
    }

    // Function to transcribe ChatGPT's audio response
    function transcribeChatGptResponse(audioUrl) {
        let audio = new Audio(audioUrl);
        let chatGptRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        chatGptRecognition.lang = 'en-US'; // Assuming English for transcription
        chatGptRecognition.interimResults = true;
        chatGptRecognition.continuous = true;

        chatGptRecognition.onresult = function(event) {
            let transcript = Array.from(event.results)
                                .map(result => result[0])
                                .map(result => result.transcript)
                                .join(' ');
            updateChatGptResponse(transcript,true);
        };
        
        chatGptRecognition.onerror = function(event) {
            console.error('Speech recognition error:', event.error);
        };

        audio.play();
        chatGptRecognition.start();

        audio.onended = function() {
            chatGptRecognition.stop(); // Stop recognition when audio ends
            URL.revokeObjectURL(audioUrl); // Clean up
        };
    }

    // Existing function to handle server response
    function handleServerResponse(data) {
        // Assuming 'data' contains the URL to the ChatGPT audio response
        transcribeChatGptResponse(data.audioUrl);
    }


    function sendTextToServer(text) {
        const serverUrl = 'http://127.0.0.1:8000/meeting/api/sendtext';
        fetch(serverUrl, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({ text: text })
        })
        .then(response => response.json())
        .then(data => {
            const audioBlob = new Blob([Uint8Array.from(atob(data.audioStreamBlob), c => c.charCodeAt(0))], {type: 'audio/mpeg'});
            const audioUrl = URL.createObjectURL(audioBlob);
            transcribeChatGptResponse(audioUrl); // Transcribe ChatGPT's audio response
        })
        .catch(error => console.error('Error:', error));
    }
    
    cameraButton.addEventListener('click', toggleCamera);
    microphoneButton.addEventListener('click', toggleMicrophone);

});
    </script>
</body>
</html>
